# -*- coding: utf-8 -*-
"""Binary Classification - Breast Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GXKV_k6Uq5ptAbMkWIVdUMD5dfHQk8pF

# Linear Models in Classification

## Introduction

According to global statistics, breast cancer
(BC) is one of the most common cancers among
women globally, accounting for the majority
of new cancer cases and cancer-related deaths
thereby, making it a major public health issue
in todayâ€™s society. As per WHO reports,there
were 2.3 million women diagnosed with breast
cancer and 685,000 deaths globally. Breast cancer treatment can be highly effective, achieving
survival probabilities of 90 per cent or higher
when the disease is identified early. Therefore,
early detection of BC improves the prognosis
and chances of survival by allowing patients to
receive timely clinical treatment. Patients may
avoid unnecessary therapies if benign tumors
are classified more precisely which makes, accurate BC diagnosis and classification of individuals into malignant or benign groups a hot
topic of research. This accurate classification
can be the difference between life and death.
Machine learning (ML) is widely regarded as the
choice of approach in BC pattern classification
and forecast modeling due to its unique benefits
in detecting essential characteristics from complex BC data sets. The use of classification and
data mining technologies to classify data is quite
effective. Particularly in the medical field, where
the methods are frequently utilized in diagnosis
and analysis.


<table>
    <tr><td>
        <img src="http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/92_7241.gif" width=300px>
        </td><td width=200px>
            $$\Rightarrow \{\text{Malignant, Benign}\}$$
        </td>
    </tr>
</table>

## Binary Classification

Cancer cells grow more chaotically than their benign counterparts. Their growth tends to be unstable, nonlinear and  ruptured. (See http://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH/PH709_Cancer/PH709_Cancer7.html)
<img src="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH/PH709_Cancer/Characteristics%20of%20Cancer%20Cells.png" width=400px>

The UW Breast Cancer Dataset (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) contains hand measured characteristics of cells as a training set and their diagnosis as _benign_ or _malignant_. 

<table><tr><td>
    <img src = "http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/91_5691.gif" width = 300>
    </td><td>
    <img src = "http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/92_7241.gif" width = 300>
    </td>
</tr></table>

The data set contains

<div class="alert alert-block alert-info">
Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.<br><br>

Number of instances: 569 <br><br>

Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)<br><br>

Attribute Information:<br><br>

1) ID number<br>
2) Diagnosis (M = malignant, B = benign)<br>


3-32) Ten real-valued features are computed for each cell nucleus:<br><br>

a) radius (mean of distances from center to points on the perimeter)<br>
b) texture (standard deviation of gray-scale values)<br>
c) perimeter<br>
d) area<br>
e) smoothness (local variation in radius lengths)<br>
f) compactness (perimeter^2 / area - 1.0)<br>
g) concavity (severity of concave portions of the contour)<br>
h) concave points (number of concave portions of the contour)<br>
i) symmetry<br>
j) fractal dimension ("coastline approximation" - 1)<br><br>

https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names
</div>

It doesn't say this very well in the documentation, but the first set of 10 features numbers in each row is the __mean__ property for all the cells in the image. The second set of 10 features is the __standard error__ and the third is the "__worst__", or most extreme. 

We start by downloading the data and the names of the columns. Note that the datafile itself is just a list of number, so we need to download the names file separately and merge the dataframes.

For an excellent kernel on data visualization for the UWBCD, take a look here: https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization. 

See also Seaborn's documentation on plotting categorical data: https://seaborn.pydata.org/tutorial/categorical.html
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')
names = ["id","diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean",
         "smoothness_mean","compactness_mean","concavity_mean","concave_points_mean",
         "symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se",
         "area_se","smoothness_se","compactness_se","concavity_se","concave" "points_se",
         "symmetry_se","fractal_dimension_se","radius_worst","texture_worst",
         "perimeter_worst","area_worst","smoothness_worst","compactness_worst",
         "concavity_worst","concave_points_worst","symmetry_worst","fractal_dimension_worst"]

data.columns=names
data.head()

"""## Exploratory analysis for Categorical Targets

Lets start with an exploratory analysis. First, we see that __diagnosis__ is our target variable and __id__ should be dropped. Let generate a few questions for our exploratory analysis:

* What does the data look like for each feature?
* What is the proportion of __malignant__ to __benign__ samples?
* What does the correlation matrix look like for mean, standard error and worst?
* Can we visualize the data in a useful way, as violin plots, box plots or swarm plots? As scatter plots?

To start, lets use the DataFrame classes built in `DataFrame.describe()` function:

* `DataFrame.describe()` returns the __count__, __mean__, __std__, __min__, __max__ and __quantiles__ for all columns of the dataframe. [Doc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)
"""

data.describe()

"""Let's split off diagnosis, and normalize the data into units of standard deviation from the mean. Recall that data frame objects have the following:


* `DataFrame.count` Count number of non-NA/null observations.
* `DataFrame.max` Maximum of the values in the object.
* `DataFrame.min` Minimum of the values in the object.
* `DataFrame.mean` Mean of the values.
* `DataFrame.std` Standard deviation of the obersvations.
* `DataFrame.select_dtypes` Subset of a DataFrame including/excluding columns based on their dtype. 
* `DataFrame.drop(columns=[])` Drop a list of columns. 
"""

## Drop Feature Columns X
X = data.drop(columns=["id","diagnosis"])

## Set Up Target Variables y
y = data["diagnosis"]

## Normalize feature data by centering on the mean and dividing by std
X = (X - X.mean())/X.std()

X.describe()

"""#### Proportion of Malignant to Benign

The proportion of malignant labels to benign labels can be computed using `DataFrame.value_counts()` and displayed using seaborns `sns.countplot()` [Doc](https://seaborn.pydata.org/generated/seaborn.countplot.html).
"""

display(y.value_counts())
sns.countplot(y)

B,M = y.value_counts()
print("Roughly ",M/(M+B),"malignant to",B/(M+B),"benign")

"""#### Correlation Matrix

Dataframes have a built in correlation matrix function, `DataFrame.corr()` and we can use seaborn to plot the heatmap with 

* `sns.heatmap(matrix, annot=True,linewidth=.5, fmt='.1f')` The heat map of a matrix `matrix`, annotated by the Pearsons coefficient, with lines between the boxes and format of the labels set to `.1f`, that is "Floating point notation truncated at 1 decimal place." For more about string formatting see for example [A Python 3 string formatting guide](https://www.programiz.com/python-programming/methods/string/format).

Lets first look at all of the correlations together, and then the correlations between the __mean__, __standard error__ and __worst__ parameters individually. 
"""

f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(X.corr(),annot=True,linewidth=.5, fmt='.1f')

"""I've set up vectors to select features for you."""

means = ["radius_mean","texture_mean","perimeter_mean","area_mean",
         "smoothness_mean","compactness_mean","concavity_mean","concave_points_mean",
         "symmetry_mean","fractal_dimension_mean"]
ses = ["radius_se","texture_se","perimeter_se",
         "area_se","smoothness_se","compactness_se","concavity_se","concave" "points_se",
         "symmetry_se","fractal_dimension_se"]
worsts = ["radius_worst","texture_worst",
         "perimeter_worst","area_worst","smoothness_worst","compactness_worst",
         "concavity_worst","concave_points_worst","symmetry_worst","fractal_dimension_worst"]

f,ax = plt.subplots(figsize=(10, 10))
sns.heatmap(X[means].corr(),annot=True,linewidth=.5, fmt='.1f')

ax.set_title("Correlation Among Means",fontsize = 20)

"""No real surprises here from a geometric standpoint. Could there be a difference between the mean correlations for malignant and benign? """

f,axes = plt.subplots(1,2,figsize=(20, 8))
I_m = y=="M"
I_b = y=="B"

sns.heatmap(X[means][I_m].corr(),annot=True,linewidth=.5, fmt='.1f',ax=axes[0])
sns.heatmap(X[means][I_b].corr(),annot=True,linewidth=.5, fmt='.1f',ax=axes[1])

axes[0].set_title("Correlation for Malignant",fontsize = 20)
axes[1].set_title("Correlation for Benign",fontsize = 20)

"""Notice that the heat maps above are not at the same scale. We can set the minimum and maximum for the colorbar, we just need to get the min and mix correlation:"""

I_m = y=="M"
I_b = y=="B"

vmin = min([X[means][I_m].corr().min().min(),X[means][I_b].corr().min().min()])
vmax = max([X[means][I_m].corr().max().max(),X[means][I_b].corr().max().max()])
print("Min:", vmin, ", Max:", vmax)

f,axes = plt.subplots(1,2,figsize=(20, 8))
I_m = y=="M"
I_b = y=="B"

sns.heatmap(X[means][I_m].corr(),annot=True,linewidth=.5, fmt='.1f',ax=axes[0],vmin=vmin,vmax=vmax)
sns.heatmap(X[means][I_b].corr(),annot=True,linewidth=.5, fmt='.1f',ax=axes[1],vmin=vmin,vmax=vmax)

axes[0].set_title("Correlation for Malignant",fontsize = 20)
axes[1].set_title("Correlation for Benign",fontsize = 20)



"""The mean is now much less correlated with the mean number of concave points. We don't need to guess, we can make this precise:"""

f,ax = plt.subplots(figsize=(10, 10))

sns.heatmap(X[means][I_m].corr() - X[means][I_b].corr(),annot=True,linewidth=.5, fmt='.1f')

ax.set_title("Correlation Difference Between Malignant and Benign",fontsize = 20)



"""## Distribution Plotting

### Box, Violin and Swarm Plots

Box, violin and swarm plots are all ways of trying to get a handle on the difference in feature distribution between the malignant and benign cells. A violin plot displays the conditional distributions next to each other for easy visual comparison. 

<img src="https://seaborn.pydata.org/_images/seaborn-violinplot-4.png">

The violin function works a little differently than other functions we have used. It takes a whole dataframe as an object and then asks us to specify which column contains the categories we want along the x-axis, which column contains the data whose distribution we want summarized, and finally which column contains the information about how the data should be labeled. 

* `sns.violinplot(x=, y=, hue=, data=data, split=True, inner="quart")` Here, split dtermins weather we will have split violins (as above) or side by side symmetric violins. The `inner = quart` line displays the quartiles one the violin plot. 

`sns.violinplot` really wants to see the data displayed in the following way:

|Color Label|X Category|Y Value|
|-----|--------|-----|
|(Smoker)|(Days)|(Tip Amount)|
|Yes| Sun| 5.40|
|No | Fri| 1.27|
|Yes| Sat| 4.41|
|Yes| Sat| 7.88|



For example, the code 

`sns.violinplot(x="diagnosis", y="radius_mean", hue="diagnosis", data=data, split=True, inner="quart")`

produces a violin plot for the variable __radius_mean__ colored by __diagnosis__. The include of `x="diagnosis"` indicates that on the $x$-axis we will be splitting the data up by the diagnosis. 
"""

plt.figure(figsize=(10,10))
sns.violinplot(x="diagnosis", y="radius_mean", hue="diagnosis", data=data, split=True, inner="quart")

"""It's an annoying feature but if we want to make a single violin like we see above we have to include a dummy category vector `x=`, where all of the category are the same.  A simple way to do this is to just pass all 1's to the category vector:"""

plt.figure(figsize=(10,10))
sns.violinplot(x=np.ones(568), y="radius_mean", hue="diagnosis", data=data, split=True, inner="quart")

"""To display all of the features side by side, we have to create a new dataframe of the form 

|Color Label|X Category|Y Value|
|-----|--------|-----|
|(diagnosis)|(Feature Name)|(Value)|
|B| radius_mean| 2.13|
|B| radius_mean| 1.27|
|M| radius_mean|-1.49|
|$\vdots$| $\vdots$| $\vdots$|
|B| area_mean| -1.32|
|M| area_mean| 0.41|

To do this, we use the `pandas.melt` function to flatten the dataframe into one long $3\times 30N$ data frame where the first column is the diagnosis, the second column is the corresponding feature and the third column is the value. First, we concatenate `y` back onto `X` and then we melt it to the proper form. 

* `pandas.melt(DataFrame, id_var=,var_name=,value_name)` Returns a dataframe of identifier varaibles while all other columns, considered measured variables (value_vars), are "unpivoted" to the row axis, leaving just two non-identifier columns, `variable` and `value` (to quote the documentation). [Doc.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)

For us, diagnosis will be the identifier variable, and we call the column of variable names "features" and the column of values "value". The violin plot is then give by letting the features (read: the variable names) run along the $x$-axis, the feature values be collected on the $y$-axis and the colors be determined by __diagnosis__.
"""

plt.figure(figsize=(10,10))

vio = pd.concat([y,X[means]],axis=1)
vio = pd.melt(vio,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

sns.violinplot(x="features", y="value", hue="diagnosis", data=vio,split=True, inner="quart")

plt.xticks(rotation=90)

"""We see that there is quiet a large difference for quite a few of the variables, including __radius_mean__, __area_mean__, __concave_points_mean__.

#### Box plots

Box plots, like violin plots, compare the differences in distributions for different labels, but they do it in a more numerical way.

<img width=400px src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/1100px-Boxplot_vs_PDF.svg.png"> [Source](https://en.wikipedia.org/wiki/Box_plot)

Here, __Q1__ is the first quartile boundary, so 25% of the data have values less than __Q1__. The inner line is the __median__ and __Q3__ is the third quartile boundary, so 75% of the data have values less than __Q3__.

Note: When the distribution is not symmetric, the whiskers will be skewed to account for the skew in the data.

<img width = 300px src="https://upload.wikimedia.org/wikipedia/commons/2/2a/Boxplots_with_skewness.png">

The `seaborn.boxplot` function uses exactly the same syntax as the `seaborn.violinplot` function.
"""

plt.figure(figsize=(10,10))

sns.boxplot(x="features", y="value", hue="diagnosis", data=vio)

plt.xticks(rotation=90)

"""It looks like __concave_points_mean__ is really starting to emerge as a favorite for indicating cancer.

#### Swarm Plots

A swarm plot is a representation of all of the data in your dataset in a set of nonoverlapping points. It gives a quick visual of how the points are distributed in a relative fashion. 

As before `sns.swarmplot` uses the same syntax as `sns.violinplot`, so once we done the work to melt our data along categories we can use seaborn to view it many different ways. 

Swarm plot may take a second to run.
"""

plt.figure(figsize=(10,10))
sns.swarmplot(x="features", y="value", hue="diagnosis", data=vio)
plt.xticks(rotation=90)

"""# Linear Regression for Binary Classifiers

Run the code below if you need to reload the data:
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')
names = ["id","diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean",
         "smoothness_mean","compactness_mean","concavity_mean","concave_points_mean",
         "symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se",
         "area_se","smoothness_se","compactness_se","concavity_se","concave" "points_se",
         "symmetry_se","fractal_dimension_se","radius_worst","texture_worst",
         "perimeter_worst","area_worst","smoothness_worst","compactness_worst",
         "concavity_worst","concave_points_worst","symmetry_worst","fractal_dimension_worst"]

data.columns=names


## Drop Feature Columns X
X = data.drop(columns=["id","diagnosis"])

## Set Up Target Variables y
y = data["diagnosis"]

## Normalize feature data by centering on the mean and dividing by std
X = (X - X.mean())/X.std()

"""We will now begin the actual fitting. For visual simplicity, lets first just consider fitting to two variables, __radius_mean__ and __concavity_mean__. """

f, ax = plt.subplots(figsize=(8,8))

I_m = y=="M"
I_b = y=="B"

plt.plot(X["radius_mean"][I_m],X["concavity_mean"][I_m],'o',label="Malignant")

## We set alpha=.5 to try to avoid masking, but some points still will be burried. 
plt.plot(X["radius_mean"][I_b],X["concavity_mean"][I_b],'o',label="Benign",alpha=.5)

plt.xlabel("radius_mean",fontsize=20)
plt.ylabel("concavity_mean",fontsize=20)
plt.legend(fontsize=15)

"""## Categorical Vector Encoding.

We need to encode `y` as a one-hot vector. That is, we assign each label to a positional vector. In this case, let

|Label|Vector|
|-----|------|
|B|[1,0]|
|M|[0,1]|

There are two ways to do this: using built in tool kit and by hand. We will use pandas built in tools here, in the exercise you will proceed by hand. 

* `pd.get_dummies(y)` Converts categorical variables into dummy index variables. 
"""

pd.get_dummies(y)

y_train = pd.get_dummies(y)
display(y_train)

"""## Linear Regression Using Sklearn

We then perform regression using sci-kit learn. The code below will reload the data:
"""

from sklearn.linear_model import LinearRegression

X_train = X[['radius_mean',"concavity_mean"]]

lr = LinearRegression()
lr.fit(X_train,y_train)

print("The r^2 score on the training data is %.3f"%(lr.score(X_train,y_train),))

"""Let's extract the parameter and plot the decision boundary. As usual, let $(x_i,y_i)$ be our training data and let $y_i\in \{0,\ldots, k-1\}$ in keeping with Pythons convention of labeling from 0. From one perspective, we've fit two linear functions using linear regression

$$
y_B = y_0 \approx f_0(X) = {\beta}_{0,0} +  X_1{\beta}_{1,0} + X_2{\beta}_{2,0}\,\hspace{3em} 
y_M = y_1 \approx f_1(X) = {\beta}_{0,1} +  X_1{\beta}_{1,1} + X_2{\beta}_{2,1}
$$

We recover a categorical fit by selecting $\hat y_i =  \underset{k}{\text{argmax}} (\hat{f}_k(x_i))$. 

To find the decision boundary, we just need to find where $\hat{f}_0(X) = \hat{f}_1(X)$. Since these are linear functions, it is easy to solve for the hyperplane

$$
X_2 = \frac{(\hat{\beta}_{1,1}-\hat{\beta}_{1,0})X_1 + \hat{\beta}_{0,1} - \hat {\beta}_{0,0}}{\hat{\beta}_{2,0}-\hat{\beta}_{2,1}}\,.
$$

In fact, $\hat f_0 = 1-\hat f_1$ for two label linear regression (__exercise__) so we could just solve $\hat f_0 = .5$, but for multilabel classification this is what generalizes. 

Extracting the $\beta$ values from the fit using `lr.coef_` and `lr.intercept_` we can plot the decision boundary on the scatter plot. 
"""

B0 = lr.intercept_
B = lr.coef_

print("The Linear Coefficients:\n", B)
print("The Intercept:", B0)

"""### Plotting Linear Fits

We want to make a plot of the decision regions. To do this we construct a dense grid of points an use the fit linear classifier to predict the label for each point. 

<div class="alert alert-block alert-warning">
Side note: It is important to remember that although in mathematical plotting $x$ runs from left to right and $y$ runs from bottom to top, in __plotting__ we often use the matrix convention, that is the top left corner of the matrix is the top left pixel, bottom right corner of the matrix is the bottom right pixel. This means that if a matrix is indexed by $(i,j)$, $i$ increases from left to right, but $j$ increases from _top to bottom_. It's because of this convention that meshgrid below returns what it does. Keep this in mind when plotting with matrices. 
</div>

We will use the `np.meshgrid` function to generate the grid of points:

* `XX, YY = np.meshgrid(XRange, YRange)` - Given a vector $x = (x_1,\ldots, x_n)$ of values `XRange` and a vector $y = (y_1,\ldots, y_m)$ of values `YRange`, meshgrid constructs all of the pairs $(x_i,y_j)$ and returns two $m\times n$ matrices: `XX` containing the $x$ coordinates of the pairs and `YY` containing the $y$ coordinates of the pairs.

For example, if $x = [1,3,4]$ and $y = [2,5]$, meshgrid will return
$$
XX = \left[
\begin{matrix}
1&3&4
\\
1&3&4
\end{matrix}
\right]
\,,\,\,\,
YY = \left[
\begin{matrix}
2&2&2
\\
5&5&5
\end{matrix}
\right]\,.
$$

Practically, this means that $(x_i,y_j) = (XX[j,i],YY[j,i])$. The index reversal is occurs because the second component of `XX` and `YY` parameterizes the horizontal directions.
"""

x_ex = [1,3,4]
y_ex = [2,5]
XX,YY = np.meshgrid(x,y_ex)

print(XX,'\n',YY)

print(XX[0,1],YY[0,1])

"""The code below shows how you can use meshgrid to generate predictions for a dense grid of values. """

f, ax = plt.subplots(figsize=(8,8))

X1 = X["radius_mean"]
X2 = X["concavity_mean"]

plt.plot(X1[I_m],X2[I_m],'o',label="Malignant")
plt.plot(X1[I_b],X2[I_b],'o',label="Benign",alpha=.5)


## We want to make a nice clean line directly across the graph as it was before
## The best way to do this is to find the limits of the graph and plot using them 

xm,xM = plt.xlim()
ym,yM = plt.ylim()

u = np.linspace(xm,xM, 2)
v = (u*(B[0,0]-B[1,0]) + B0[0]-B0[1])/(B[1,1]-B[0,1])
plt.plot(u,v,label="Decision Boundary",color="black")


## We also may also want to color in the side of the decicion boundry we're
## Labeling each point. One way to do this is using a mesh grid, and then using
## an indexon the equation from before

XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) 

z1 = YY >(XX*(B[0,0]-B[1,0]) + B0[0]-B0[1])/(B[1,1]-B[0,1])
z2 = YY <(XX*(B[0,0]-B[1,0]) + B0[0]-B0[1])/(B[1,1]-B[0,1])

plt.plot(XX[z1],YY[z1],',',color="C0")
plt.plot(XX[z2],YY[z2],',',color="C1")

## We now reset the x and y limits to make sure our view is centered tightly
## around the data. 

plt.xlabel("radius_mean",fontsize=20)
plt.ylabel("concavity_mean",fontsize=20)
plt.legend(fontsize=15)

ax.set_xlim([xm, xM])
ax.set_ylim([ym, yM])

"""## (Linear) Quadratic Discriminant Analysis (QDA)

Recall that in quadratic discriminant analysis, we assume a Gaussian distribution for each label

$$
y_k \approx f_k(X) = \big[(2\pi)^p |\mathbf{\Sigma}| \big]^{-\frac12}\exp\left(\,-\frac12(x-\mu_k)^T\mathbf{\Sigma}^{-1}(x-\mu_k)  \,\right)\,,
$$

Where $\mu$ is the center of the label distribution, $\mathbf{\Sigma}$ is the covariance matrix. The discriminant functions are then quadratic, and given by 

$$
\delta_k(x) = -\frac12\log|\mathbf{\Sigma}_k| - \frac12 (x-\mu_k)^T\mathbf{\Sigma}_k^{-1}(x-\mu_k) + \log \pi_k\,.
$$



<div class="alert alert-block alert-warning">
Sci-kit learn has a QDA library, but if you need to you can estimate the parameters by <br><br>
$\hat \pi_k = N_k/N$, where $N_k$ is the number of observations of $k$. Stored in `qda.priors_`. <br>
$\hat\mu_k  = \frac{1}{N_k}\sum_{y_i = k} x_i$ is the mean of $k$ observations. Stored in `qda.means_`.<br>
$\hat{\mathbf{\Sigma}} = \frac{1}{N-K}\sum_{k=1}^K\sum_{y_i=k}||x_i - \hat \mu_k||^2$ estimates covariance. Stored in `qda.covariance_`.<br>
</div>

Using sci-kit learn's `QuadraticDiscriminantAnalysis` class from the `discriminant_analysis` library, we can fit the function as before. You can use the code below to compute the linear decision boundary by just changing the function call. 
"""

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

qda = QuadraticDiscriminantAnalysis(store_covariance=True)
qda.fit(X_train, y)
print("Score: %.3f"%qda.score(X_train,y))



"""This seems to do a lot better than regression, but can we trust it? Indeed, this is scoring itself using a different metric to regression. Regression uses least squares while QDA used mean accuracy (mean number of correct prediction). 

__Question:__ Which score would you expect regression to the best with respect to, the $r^2$ score or the mean accuracy?

We will now use fit object's `predict` function to generate the background labels.
"""

f, ax = plt.subplots(figsize=(8,8))

X1 = X["radius_mean"]
X2 = X["concavity_mean"]

plt.plot(X1[I_m],X2[I_m],'o',label="Malignant")
plt.plot(X1[I_b],X2[I_b],'o',label="Benign",alpha=.5)

## As before we generate a meshgrid, but now we use qda.predict to guess at the label. 

xm,xM = plt.xlim()
ym,yM = plt.ylim()

XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) 

## We now form a 10000x2 array of the (x,y) coordiantes for each point by reshaping
## the XX and YY matricies and pasting them together. We need to feed a Nx2 vector
## into the qda.predict function, otherwise it will think we have too many features.
## We can reshape it later to get our grid back

grid=np.concatenate([XX.reshape(-1,1),YY.reshape(-1,1)],axis=1)

ZZ = qda.predict(grid).reshape(XX.shape)  ## We predict, and reshape back to the origional grid

z1 = ZZ == 'M'
z2 = ZZ == 'B'

plt.plot(XX[z1],YY[z1],',',color="C0")
plt.plot(XX[z2],YY[z2],',',color="C1")

## We now reset the x and y limits to make sure our view is centered tightly
## around the data. 

plt.xlabel("radius_mean",fontsize=20)
plt.ylabel("concavity_mean",fontsize=20)
plt.legend(fontsize=15)

ax.set_xlim([xm, xM])
ax.set_ylim([ym, yM])

"""Plotting the decision boundary for QDA is more difficult than in the linear case. If you are interested in a well worked out example the official documentation has one here: https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py

#### Exercise:

Write a formula for the QDA decision boundary in terms of the mean, prior, and covariance. Implement your formula in Python.

## Linear discriminant analysis

Compare the above the LDA here below. We have only changed two things: First, we call `LinearDiscriminantAnalysis` instead of quadratic and second we have added the discriminant line from the regression analysis in. Notice that both match up.
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(store_covariance=True)
lda.fit(X_train, y)

print("Score: %.3f"%qda.score(X_train,y))

f, ax = plt.subplots(figsize=(8,8))

X1 = X["radius_mean"]
X2 = X["concavity_mean"]

plt.plot(X1[I_m],X2[I_m],'o',label="Malignant")
plt.plot(X1[I_b],X2[I_b],'o',label="Benign",alpha=.5)

## As before we generate a meshgrid, but now we use qda.predict to guess at the label. 

xm,xM = plt.xlim()
ym,yM = plt.ylim()

XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) 

## We now form a 10000x2 array of the (x,y) coordiantes for each point by reshaping
## the XX and YY matricies and pasting them together. We need to feed a Nx2 vector
## into the qda.predict function, otherwise it will think we have too many features.
## We can reshape it later to get our grid back

grid=np.concatenate([XX.reshape(-1,1),YY.reshape(-1,1)],axis=1)

ZZ = qda.predict(grid).reshape(XX.shape)  ## We predict, and reshape back to the origional grid

z1 = ZZ == 'M'
z2 = ZZ == 'B'

plt.plot(XX[z1],YY[z1],',',color="C0")
plt.plot(XX[z2],YY[z2],',',color="C1")

plt.plot(u,v,label="Decision Boundary",color="black")

## We now reset the x and y limits to make sure our view is centered tightly
## around the data. 

plt.xlabel("radius_mean",fontsize=20)
plt.ylabel("concavity_mean",fontsize=20)
plt.legend(fontsize=15)

ax.set_xlim([xm, xM])
ax.set_ylim([ym, yM])

"""## Logistic regression

The logistic regression classifier has logistic discriminant functions

$$
y_j \approx \mathbb{P}(G=j|X=x) = \frac{\exp(\beta_{j,0}+x^T\beta_j)}{1+\sum_{\ell=1}^{K-1}\exp(\beta_{\ell,0} + x^T\beta_\ell)}\,,\hspace{1em} \forall j=1,\ldots, K-1\,,
$$
and 
$$
y_K \approx \mathbb{P}(G=K|X=x)  = \frac{1}{1+\sum_{\ell=1}^{K-1}\exp(\beta_{\ell,0} + x^T\beta_\ell)}\,.
$$

Again, sci-kit learn has a built in classifier in `sklearn.linear_model`, the `LogisticRegression` class [Doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).
"""

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train,y)

print("Score: %.3f"%clf.score(X_train,y))

f, ax = plt.subplots(figsize=(8,8))

X1 = X["radius_mean"]
X2 = X["concavity_mean"]

plt.plot(X1[I_m],X2[I_m],'o',label="Malignant")
plt.plot(X1[I_b],X2[I_b],'o',label="Benign",alpha=.5)

## As before we generate a meshgrid, but now we use qda.predict to guess at the label. 

xm,xM = plt.xlim()
ym,yM = plt.ylim()

XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) 

## We now form a 10000x2 array of the (x,y) coordiantes for each point by reshaping
## the XX and YY matricies and pasting them together. We need to feed a Nx2 vector
## into the qda.predict function, otherwise it will think we have too many features.
## We can reshape it later to get our grid back

grid=np.concatenate([XX.reshape(-1,1),YY.reshape(-1,1)],axis=1)

ZZ = clf.predict(grid).reshape(XX.shape)  ## We predict, and reshape back to the origional grid

z1 = ZZ == 'M'
z2 = ZZ == 'B'

plt.plot(XX[z1],YY[z1],',',color="C0")
plt.plot(XX[z2],YY[z2],',',color="C1")

plt.plot(u,v,label="Decision Boundary",color="black")

## We now reset the x and y limits to make sure our view is centered tightly
## around the data. 

plt.xlabel("radius_mean",fontsize=20)
plt.ylabel("concavity_mean",fontsize=20)
plt.legend(fontsize=15)

ax.set_xlim([xm, xM])
ax.set_ylim([ym, yM])

"""## Cross Validation with Sci-Kit Learn

We see that logistic regression actually does a bit _worse_ than linear regression. This is probably not surprising, we know that linear regression should perform well when only being compared to the dataset itself. We would expect to gain something if we split the data and tried cross validation. We will use the `train_test_split` library from sci-kit learns `model_selection` library.

* `train_test_split(X,y, test_size=, random_state)` Splits the `X` and `y` data into four pieces: `X_train`, `X_test`, `y_train`, and `y_test`. You may split by number or by percentage. You may use `random_state` to specify a random seed so that you can recover the splitting. if need be. 

It's a good exercise to see how the relative prediction accuracy changes as we change the `test_size` parameter. 
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X[["radius_mean","concavity_mean"]], 
                                                    y, test_size=0.4)




## Linear Regression Analysis

lda = LinearRegression()
lda.fit(X_train, pd.get_dummies(y_train))
y_hat = np.argmax(lda.predict(X_test), axis=1)
y_true = np.argmax(np.matrix(pd.get_dummies(y_test)), axis=1)
print("Linear Regression Score: %.3f"%accuracy_score(y_hat,y_true))

## Linear Discriminant Analysis

lda = LinearDiscriminantAnalysis(store_covariance=True)
lda.fit(X_train, y_train)
print("LDA Score: %.3f"%lda.score(X_test,y_test))

## Quadratic Discriminant Analysis

qda = QuadraticDiscriminantAnalysis(store_covariance=True)
qda.fit(X_train, y_train)
print("QDA Score: %.3f"%qda.score(X_test,y_test))

## Logisitic Regression

clf = LogisticRegression()
clf.fit(X_train,y_train)
print("Logistic Regression Score: %.3f"%clf.score(X_test,y_test))



"""## Building Utility Functions:

You will notice that we have repeated a lot of the same code in the three examples above. Once we've gotten a piece of code running the way we want it it can be very useful to write it up as a function to be used later. In the code below, we will show how to write the graphing of the decision boundary in a function `predict_labels`. Since we may want to compare decision boundaries, we need to pass the function three things: The data `X` and `y`, the classifier, and the axis to be written to:
"""

def predict_labels(X,y,ax,pred):
    [col0,col1] = X.columns
    labels = set(y)
    
    ## Plot Datapoints By Label
    for l in labels:
        X_l = X[y==l]
        ax.plot(X_l[col0],X_l[col1],'o',label=l,alpha=.5)
        
    ax.set_xlabel(col0,fontsize=20)
    ax.set_ylabel(col1,fontsize=20)
    ax.legend(fontsize=15)
    
    ### Predict on Grid
    xm,xM = ax.get_xlim()
    ym,yM = ax.get_ylim()
    XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) 
    
    grid=np.concatenate([XX.reshape(-1,1),YY.reshape(-1,1)],axis=1)
    ZZ = pred.predict(grid).reshape(XX.shape)  ## We predict, and reshape back to the origional grid
    z1 = ZZ == 'M'
    z2 = ZZ == 'B'
    ax.plot(XX[z1],YY[z1],',',color="C0")
    ax.plot(XX[z2],YY[z2],',',color="C1")
    
    ax.set_xlim([xm, xM])
    ax.set_ylim([ym, yM])
    ax.set_title(type(pred).__name__)
    

    
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
X = data[["radius_mean","texture_mean"]]
clf.fit(X,y)
    
f, ax = plt.subplots(figsize=(5,5))   
predict_labels(X,y,ax,clf)

## Liner Regression vis Linear Discriminant Analysis
lda = LinearDiscriminantAnalysis(store_covariance=True)
lda.fit(X_train, y_train)

## Quadratic Discriminant Analysis
qda = QuadraticDiscriminantAnalysis(store_covariance=True)
qda.fit(X_train, y_train)

## Logisitic Regression
clf = LogisticRegression()
clf.fit(X_train,y_train)

## Plotting:
f, axes = plt.subplots(1,3, figsize=(15,5))
axes = axes.ravel()

for i, pred in enumerate([lda, qda, clf]):
    predict_labels(X_train,y,axes[i],pred)

plt.tight_layout()